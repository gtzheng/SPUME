use_val: False
use_group_label: False # choose whether to use the ground-truth group labels for training
fast_train: 0 # if 0, use all the training data per epoch to calculate spuriousness scores; otherwise, randomly sample fast_train batches per epoch
random_sampler: False # randomly sample meta-learning tasks
pretrained: True # choose whether to use ImageNet-pretrained weights
test: False # If True, skip the training
batch_size : 64 # batch size
num_supp : 10 # number of training shots
num_query : 10 # number of query samples
topk: 1 # select how many attributes (union, samples have at least one of these attributes) are used to select samples
num_epochs : 100 # training epochs
num_episode : 20 # how many task batches are used in each epoch
task_num: 4 # how many tasks are contained in each task batch
n_classes: 2 # number of classes
vlm: "vit-gpt2" # blip or vit-gpt2
dataset: "waterbirds" # celeba, waterbirds, imagenet-9, nico
backbone: "resnet50" # model architecture
score_func: "tanh-abs-log" # score function, choose from {tanh-abs-log, tanh-log, abs-log, log, abs-diff, diff, exp-diff, exp-abs-diff}, default: tanh-log
scheduler: "cosine" # multistep
milestones: [20, 40, 60, 80]
gamma: 0.2
alpha: 1.0 # strength of the meta-learning loss
lr : 1.e-3 # learning rate
augmentation : True # whether to use data augmentation
temp : 5.0 # temperatuer used at the nearest neighbor classifier with cosine distance
val_threshold_num : 50 # minimum number of samples used for calculating the pseudo worst validation accuracy
tag: "exp"


save_folder: "/path/meta_spurious_exprs/" # where to save the experimental results

data_folder: "/path/data/waterbird_complete95_forest2water2"
vit_gpt2_attr_embed_path: "/path/data/waterbird_complete95_forest2water2/vit-gpt2_img_embeddings.pickle"
vit_gpt2_vocab_path: "/path/data/waterbird_complete95_forest2water2/vit-gpt2_vocab.pickle"
blip_attr_embed_path: "/path/data/waterbird_complete95_forest2water2/blip_img_embeddings.pickle"
blip_vocab_path: "/path/data/waterbird_complete95_forest2water2/blip_vocab.pickle"
num_workers: 8 # number of workers used in data loading