use_val: False
use_group_label: False # choose whether to use the ground-truth group labels for training
fast_train: 20 # if 0, use all the training data per epoch to calculate spuriousness scores; otherwise, randomly sample fast_train batches per epoch
random_sampler: False # randomly sample meta-learning tasks
pretrained: True # choose whether to use ImageNet-pretrained weights
test: False # If True, skip the training
batch_size : 256 # batch size
topk: 1
num_supp : 10 # number of training shots
num_query : 10 # number of query samples
num_epochs : 50 # training epochs
num_episode : 20 # how many task batches are used in each epoch
task_num: 2 # how many tasks are contained in each task batch
n_classes: 9 # number of classes
dataset: "imagenet-9" # celeba, waterbirds, imagenet-9, nico
backbone: "resnet18" # model architecture
score_func: "tanh-abs-log" # score function, choose from {tanh-abs-log, tanh-log, abs-log, log, abs-diff, diff}
vlm: "blip"
alpha: 1.0 # strength of the meta-learning loss
lr : 1.e-4 # learning rate
scheduler: "cosine"
milestones: [50, 90, 130, 170]
gamma: 0.2
augmentation : True # whether to use data augmentation
temp : 50 # temperatuer used at the nearest neighbor classifier with cosine distance
val_threshold_num : 50 # minimum number of samples used for calculating the pseudo worst validation accuracy
tag: "exp1"

save_folder: "/path/meta_spurious_exprs/" # where to save the experimental results

im9_data_folder: "/path/data/imagenet"
imA_data_folder: "/path/data/imagenet-a"
im9_cluster_file: "/path/data/imagenet/9class_imagenet_val.csv"
vit_gpt2_attr_embed_path: "/path/data/imagenet/vit-gpt2_img_embeddings.pickle"
vit_gpt2_vocab_path: "/path/data/imagenet/vit-gpt2_vocab.pickle"
blip_attr_embed_path: "/path/data/imagenet/blip_img_embeddings.pickle"
blip_vocab_path: "/path/data/imagenet/blip_vocab.pickle"
num_workers: 8 # number of workers used in data loading