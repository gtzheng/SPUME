use_val: False
use_group_label: False # choose whether to use the ground-truth group labels for training
fast_train: 20 # if 0, use all the training data per epoch to calculate spuriousness scores; otherwise, randomly sample fast_train batches per epoch
random_sampler: False # randomly sample meta-learning tasks
pretrained: True # choose whether to use ImageNet-pretrained weights
test: False # If True, skip the training
batch_size : 256 # batch size
topk: 1
num_supp : 10 # number of training shots
num_query : 10 # number of query samples
num_epochs : 100 # training epochs
num_episode : 20 # how many task batches are used in each epoch
task_num: 4 # how many tasks are contained in each task batch
n_classes: 2 # number of classes
dataset: "celeba" # celeba, waterbirds, imagenet-9, nico
backbone: "resnet50" # model architecture
vlm: "blip" # blip or vit-gpt2
score_func: "tanh-abs-log" # score function, choose from {tanh-abs-log, tanh-log, abs-log, log, abs-diff, diff}
scheduler: "cosine" # multistep, cosine
milestones: [40, 60, 80]
gamma: 0.1
alpha: 1.0 # strength of the meta-learning loss
lr : 1.e-3 # learning rate
augmentation : True # whether to use data augmentation
temp : 10.0 # temperatuer used at the nearest neighbor classifier with cosine distance
val_threshold_num : 50 # minimum number of samples used for calculating the pseudo worst validation accuracy
tag: "exp1"


save_folder: "/path/meta_spurious_exprs/" # where to save the experimental results
data_folder: "/path/data/celeba/img_align_celeba"
vit_gpt2_attr_embed_path: "/path/data/celeba/img_align_celeba/vit-gpt2_img_embeddings.pickle"
vit_gpt2_vocab_path: "/path/data/celeba/img_align_celeba/vit-gpt2_vocab.pickle"
blip_attr_embed_path: "/path/data/celeba/img_align_celeba/blip_img_embeddings.pickle"
blip_vocab_path: "/path/data/celeba/img_align_celeba/blip_vocab.pickle"
num_workers: 8 # number of workers used in data loading
